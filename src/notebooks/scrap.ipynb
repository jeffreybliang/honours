{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "def pytorch3d_to_trimesh(p3d_mesh):\n",
    "    # Get vertices and faces from the PyTorch3D mesh\n",
    "    vertices = p3d_mesh.verts_packed().detach().cpu().numpy()\n",
    "    faces = p3d_mesh.faces_packed().detach().cpu().numpy()\n",
    "\n",
    "    # Create a Trimesh object\n",
    "    return trimesh.Trimesh(vertices=vertices, faces=faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to have meshes with different number of vertices:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equality_constraints(self, xs: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Enforces volume constraint\n",
    "    Assumes same number of vertices in each projected mesh currently\n",
    "\n",
    "    Args:\n",
    "        xs (tensor): vertices of original mesh, B x V x 3\n",
    "        y (tensor): vertices of projected mesh, B x V x 3\n",
    "    \"\"\"\n",
    "    n_batches = len(xs)\n",
    "    verts = y.view(n_batches,-1,3) # (B, V, 3)\n",
    "\n",
    "    faces = self.src.faces_padded().view(n_batches, -1, 3).detach()  # (B, max F_i, 3)\n",
    "    _, n_faces, _ = faces.size()\n",
    "    batch_indices = torch.arange(n_batches)[:, None, None].expand(-1, n_faces, 3).detach() # detached\n",
    "\n",
    "    face_vertices = verts[batch_indices, faces]  # (B, F, 3, 3)\n",
    "    # Calculate volume\n",
    "    v0, v1, v2 = face_vertices[:, :, 0, :], face_vertices[:, :, 1, :], face_vertices[:, :, 2, :]\n",
    "    cross = torch.cross(v1, v2, dim=-1)  # (B, F, 3)\n",
    "    face_volumes = torch.einsum('bfi,bfi->bf', v0, cross) / 6.0  # (B, F)\n",
    "\n",
    "    volumes = torch.sum(face_volumes, dim=1).abs() # (B,)\n",
    "    return volumes  # (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signed_tet_volume(face_vertices: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute signed tetrahedron volumes for a batch of faces.\n",
    "\n",
    "    Args:\n",
    "        face_vertices (torch.Tensor): Tensor of shape (F, 3, 3), where\n",
    "                                      F is the number of faces, and each face\n",
    "                                      consists of 3 vertices in 3D.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (F,) containing signed volumes.\n",
    "    \"\"\"\n",
    "    v0, v1, v2 = face_vertices[:, 0, :], face_vertices[:, 1, :], face_vertices[:, 2, :]\n",
    "    \n",
    "    # Compute determinant of the 3x3 matrix [v0, v1, v2]\n",
    "    volumes = torch.det(torch.stack([v0, v1, v2], dim=-1)) / 6.0  # Shape: (F,)\n",
    "\n",
    "    return volumes\n",
    "\n",
    "def get_volume_batch(meshes: Meshes):\n",
    "    verts_packed = meshes.verts_packed()  # (sum(V_i), 3)\n",
    "    faces_packed = meshes.faces_packed()  # (sum(F_i), 3)\n",
    "    mesh_to_face = meshes.mesh_to_faces_packed_first_idx()  # Index of first face per mesh\n",
    "    n_meshes = len(meshes)\n",
    "    volumes = torch.zeros(n_meshes, device=verts_packed.device)\n",
    "\n",
    "    for i in range(n_meshes):\n",
    "        start = mesh_to_face[i]\n",
    "        end = start + meshes.num_faces_per_mesh()[i]\n",
    "        face_vertices = verts_packed[faces_packed[start:end]]  # (F, 3, 3)\n",
    "        volumes[i] = get_signed_tet_volume(face_vertices).sum()  # Sum over all faces\n",
    "\n",
    "    return volumes.abs()  # Returns a tensor of shape (num_meshes,)\n",
    "\n",
    "def least_squares(u0, target):\n",
    "    \"\"\"\n",
    "    u0 are vertices\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(u0):\n",
    "        u0 = torch.tensor(u0)\n",
    "    if not torch.is_tensor(target):\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "    res = torch.square(u0 - target).sum()\n",
    "    print(\"lstsq\", res)\n",
    "    return res.double()\n",
    "\n",
    "def least_squares_grad(u0, target):\n",
    "    if torch.is_tensor(u0):\n",
    "        u0 = u0.detach().clone()\n",
    "    else:\n",
    "        u0 = torch.tensor(u0)\n",
    "        \n",
    "    if torch.is_tensor(target):\n",
    "        target = target.detach().clone()\n",
    "    else:\n",
    "        target = torch.tensor(target)\n",
    "        \n",
    "    # Ensure that u0 requires gradients\n",
    "    u0.requires_grad = True\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        res = torch.square(u0 - target).sum()\n",
    "\n",
    "    # Compute the gradient\n",
    "    obj_grad = torch.autograd.grad(res, u0)[0]\n",
    "    print(\"obj_grad\", obj_grad)\n",
    "    return obj_grad.double()\n",
    "\n",
    "def get_volume(u, faces, init_vol):\n",
    "    if not torch.is_tensor(u):\n",
    "        u = torch.tensor(u)\n",
    "    if not torch.is_tensor(faces):\n",
    "        faces = torch.tensor(faces)\n",
    "    if not torch.is_tensor(init_vol):\n",
    "        init_vol = torch.tensor(init_vol)\n",
    "    \n",
    "    vertices = u.view(-1,3)\n",
    "    face_vertices = vertices[faces]  # (F, 3, 3)\n",
    "    volume = get_signed_tet_volume(face_vertices).sum()\n",
    "    res = volume.abs() - init_vol\n",
    "    print(\"vol\", res)\n",
    "    return res.double()\n",
    "\n",
    "def get_volume_grad(u, faces, init_vol):\n",
    "    if not torch.is_tensor(u):\n",
    "        u = torch.tensor(u, dtype=torch.float64, requires_grad=True)\n",
    "    else:\n",
    "        u = u.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    if not torch.is_tensor(faces):\n",
    "        faces = torch.tensor(faces, dtype=torch.long)\n",
    "    \n",
    "    if not torch.is_tensor(init_vol):\n",
    "        init_vol = torch.tensor(init_vol, dtype=torch.float64)\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        vertices = u.view(-1, 3)\n",
    "        face_vertices = vertices[faces]  # (F, 3, 3)\n",
    "        volume = get_signed_tet_volume(face_vertices).sum()\n",
    "        res = volume.abs() - init_vol\n",
    "    \n",
    "    volume_grad = torch.autograd.grad(res, u, retain_graph=True)[0]\n",
    "    print(\"volume grad\", volume_grad)\n",
    "    \n",
    "    return volume_grad.double()\n",
    "\n",
    "def project(meshes: Meshes, targets: Meshes):\n",
    "    n_batches = len(meshes)\n",
    "    n_vtxs = len(meshes[0].verts_packed())\n",
    "    results = torch.zeros(n_batches, n_vtxs, 3, dtype=torch.double)\n",
    "    losses = torch.zeros(n_batches, 1, dtype=torch.double)\n",
    "    for batch_number, mesh in enumerate(meshes):\n",
    "        init_vol = get_volume_batch(mesh).double().detach().cpu().numpy()\n",
    "        print(\"batch number\", batch_number)\n",
    "        vertices = mesh.verts_packed().flatten().detach().numpy().astype(np.float64)\n",
    "        faces = mesh.faces_packed().detach().numpy().astype(np.int64)\n",
    "        target_vtx = targets[batch_number].verts_packed().flatten().detach().numpy().astype(np.float64)\n",
    "        eq_const = {\n",
    "            'type': 'eq',\n",
    "            'fun' : lambda u: get_volume(u, faces, init_vol).cpu().numpy().astype(np.float64),\n",
    "            'jac' : lambda u: get_volume_grad(u, faces, init_vol).cpu().numpy().astype(np.float64)\n",
    "        }\n",
    "        print(\"starting optimisation\")\n",
    "        res = opt.minimize(\n",
    "            lambda u: least_squares(u, target_vtx).detach().cpu().numpy().astype(np.float64),\n",
    "            vertices, \n",
    "            jac=lambda u: least_squares_grad(u, target_vtx).cpu().numpy().astype(np.float64),\n",
    "            method='SLSQP', \n",
    "            constraints=[eq_const],\n",
    "            options={'ftol': 1e-4, 'disp': True, 'maxiter': 100}\n",
    "        )   \n",
    "        print(\"finished\")\n",
    "        if not res.success:\n",
    "            print(\"FIT failed:\", res.message)\n",
    "        results[batch_number] = torch.tensor(res.x, dtype=torch.double, requires_grad=True).view(-1,3)\n",
    "        losses[batch_number] = torch.tensor(res.fun, dtype=torch.double, requires_grad=False)\n",
    "    return results, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch3d.loss import chamfer_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm is valid: 2\n",
      "p1 shape: torch.Size([4, 25, 2])\n",
      "p2 shape: torch.Size([4, 25, 2])\n",
      "lengths1 shape: torch.Size([4]) tensor([25, 25, 25, 25])\n",
      "lengths2 shape: torch.Size([4]) tensor([25, 25, 25, 25])\n",
      "K: 1\n",
      "version: -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported TypeMeta in ATen:  (please report this error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand((batch_size,num_points, dim))  \u001b[39m# (N, P2, D)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Compute Chamfer distance\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m loss, _ \u001b[39m=\u001b[39m chamfer_distance(x, y)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mChamfer Distance:\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Documents/Honours/venv/lib/python3.12/site-packages/pytorch3d/loss/chamfer.py:249\u001b[0m, in \u001b[0;36mchamfer_distance\u001b[0;34m(x, y, x_lengths, y_lengths, x_normals, y_normals, weights, batch_reduction, point_reduction, norm, single_directional, abs_cosine)\u001b[0m\n\u001b[1;32m    246\u001b[0m x, x_lengths, x_normals \u001b[39m=\u001b[39m _handle_pointcloud_input(x, x_lengths, x_normals)\n\u001b[1;32m    247\u001b[0m y, y_lengths, y_normals \u001b[39m=\u001b[39m _handle_pointcloud_input(y, y_lengths, y_normals)\n\u001b[0;32m--> 249\u001b[0m cham_x, cham_norm_x \u001b[39m=\u001b[39m _chamfer_distance_single_direction(\n\u001b[1;32m    250\u001b[0m     x,\n\u001b[1;32m    251\u001b[0m     y,\n\u001b[1;32m    252\u001b[0m     x_lengths,\n\u001b[1;32m    253\u001b[0m     y_lengths,\n\u001b[1;32m    254\u001b[0m     x_normals,\n\u001b[1;32m    255\u001b[0m     y_normals,\n\u001b[1;32m    256\u001b[0m     weights,\n\u001b[1;32m    257\u001b[0m     point_reduction,\n\u001b[1;32m    258\u001b[0m     norm,\n\u001b[1;32m    259\u001b[0m     abs_cosine,\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m single_directional:\n\u001b[1;32m    262\u001b[0m     loss \u001b[39m=\u001b[39m cham_x\n",
      "File \u001b[0;32m~/Documents/Honours/venv/lib/python3.12/site-packages/pytorch3d/loss/chamfer.py:111\u001b[0m, in \u001b[0;36m_chamfer_distance_single_direction\u001b[0;34m(x, y, x_lengths, y_lengths, x_normals, y_normals, weights, point_reduction, norm, abs_cosine)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[39mreturn\u001b[39;00m ((x\u001b[39m.\u001b[39msum((\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)) \u001b[39m*\u001b[39m weights) \u001b[39m*\u001b[39m \u001b[39m0.0\u001b[39m, (x\u001b[39m.\u001b[39msum((\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)) \u001b[39m*\u001b[39m weights) \u001b[39m*\u001b[39m \u001b[39m0.0\u001b[39m)\n\u001b[1;32m    109\u001b[0m cham_norm_x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mnew_zeros(())\n\u001b[0;32m--> 111\u001b[0m x_nn \u001b[39m=\u001b[39m knn_points(x, y, lengths1\u001b[39m=\u001b[39;49mx_lengths, lengths2\u001b[39m=\u001b[39;49my_lengths, norm\u001b[39m=\u001b[39;49mnorm, K\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    112\u001b[0m cham_x \u001b[39m=\u001b[39m x_nn\u001b[39m.\u001b[39mdists[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m]  \u001b[39m# (N, P1)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m is_x_heterogeneous:\n",
      "File \u001b[0;32m~/Documents/Honours/venv/lib/python3.12/site-packages/pytorch3d/ops/knn.py:244\u001b[0m, in \u001b[0;36mknn_points\u001b[0;34m(p1, p2, lengths1, lengths2, norm, K, version, return_nn, return_sorted)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m lengths2 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     lengths2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((p1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],), P2, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64, device\u001b[39m=\u001b[39mp1\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 244\u001b[0m p1_dists, p1_idx \u001b[39m=\u001b[39m _knn_points\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    245\u001b[0m     p1, p2, lengths1, lengths2, K, version, norm, return_sorted\n\u001b[1;32m    246\u001b[0m )\n\u001b[1;32m    248\u001b[0m p2_nn \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m return_nn:\n",
      "File \u001b[0;32m~/Documents/Honours/venv/lib/python3.12/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Honours/venv/lib/python3.12/site-packages/pytorch3d/ops/knn.py:85\u001b[0m, in \u001b[0;36m_knn_points.forward\u001b[0;34m(ctx, p1, p2, lengths1, lengths2, K, version, norm, return_sorted)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mversion:\u001b[39m\u001b[39m\"\u001b[39m, version)\n\u001b[1;32m     83\u001b[0m p1, p2, lengths1, lengths2 \u001b[39m=\u001b[39m p1\u001b[39m.\u001b[39mcpu(), p2\u001b[39m.\u001b[39mcpu(), lengths1\u001b[39m.\u001b[39mcpu(), lengths2\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m---> 85\u001b[0m idx, dists \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39;49mknn_points_idx(p1, p2, lengths1, lengths2, norm, K, version)\n\u001b[1;32m     87\u001b[0m \u001b[39m# Check the outputs of knn_points_idx\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39midx shape:\u001b[39m\u001b[39m\"\u001b[39m, idx\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsupported TypeMeta in ATen:  (please report this error)"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_points = 25\n",
    "dim = 2\n",
    "\n",
    "# Generate two random batches of point clouds\n",
    "x = torch.rand((batch_size,num_points, dim))  # (N, P1, D)\n",
    "y = torch.rand((batch_size,num_points, dim))  # (N, P2, D)\n",
    "\n",
    "# Compute Chamfer distance\n",
    "loss, _ = chamfer_distance(x, y)\n",
    "print(\"Chamfer Distance:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3c5e5b74b02413c78a36d1154fc177d57260c9d451ea89539c66d8b3bbae5e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
